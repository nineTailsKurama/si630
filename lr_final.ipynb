{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 13331)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "import emot\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "x_train = open('X_train.txt').readlines()\n",
    "y_train = open('Y_train.txt').readlines()\n",
    "x_test = open('X_test.txt').readlines()\n",
    "\n",
    "y_train = [ 0 if x == \"0\\n\" else 1 for x in y_train]  \n",
    "y_train = np.matrix([y_train])#np.transpose(np.matrix([y_train]))\n",
    "        # self.distribution_aggresive = {}\n",
    "        # self.distributive_not_aggresive = {}\n",
    "final = []\n",
    "count_in_doc = {}\n",
    "l = []\n",
    "word_dict = {}\n",
    "        # self.document_count()\n",
    "\n",
    "def tokenize( s):\n",
    "    #Lowercase every string\n",
    "    s = s.lower()\n",
    "    #convert emojis to text\n",
    "    s = emoji.demojize(s, delimiters=(\"\", \"\"))\n",
    "    s = emoji.demojize(s, delimiters=(\"\", \"\"))\n",
    "    answ = emot.emoticons(s)\n",
    "    if(str(type(answ))== \"<class 'list'>\"):\n",
    "        answ = answ[0]\n",
    "    if(answ['flag']):\n",
    "        # s = s.replace(answ['value'],answ['mean'])\n",
    "        j = 0\n",
    "        for i in answ['value']:\n",
    "            s = s.replace(i, \" \"+ answ['mean'][j].split()[-1])\n",
    "            j = j+1\n",
    "    # s = s.replace(ans['value'],ans['mean'])\n",
    "    # Remove punctuation and all weird characters\n",
    "    s = re.sub(\"\\W\",\" \", s)\n",
    "    # Special character cleaning\n",
    "    s = re.sub(\"\\s\",\" \", s)\n",
    "    return list(s.split())\n",
    "\n",
    "def document_count():\n",
    "    for x in x_train:\n",
    "        for i in tokenize(x):\n",
    "            if i not in count_in_doc:\n",
    "                word_dict[i] = []\n",
    "                count_in_doc[i] = [0]*12000\n",
    "    count_in_doc[\"BIAS\"]= [1]*12000\n",
    "    for k in range(len(x_train)):\n",
    "        i_long = tokenize(x_train[k])\n",
    "        for i in i_long:\n",
    "            count_in_doc[i][k] = (i_long.count(i))\n",
    "    l = []\n",
    "    for key, value in count_in_doc.items():\n",
    "        l.append(count_in_doc[key])\n",
    "    l = np.transpose(np.matrix(l))\n",
    "    print(l.shape)\n",
    "    final = l \n",
    "    return final\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def log_likelihood( b):\n",
    "    pass\n",
    "\n",
    "def zero_or_one(x):\n",
    "    if(x>(0.50)):\n",
    "        return 1 \n",
    "    return 0\n",
    "sigmoid_vec = np.vectorize(sigmoid)\n",
    "\n",
    "final = document_count()\n",
    "# print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train =np.transpose(y_train)\n",
    "print(y_train)\n",
    "\n",
    "zero_or_one_vec =   np.vectorize(zero_or_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13331, 1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def middle_layer_1__inital_matrix(): \n",
    "    number_of_neurons = 10\n",
    "    layer1_weights =  np.zeros( final.shape[1]) #np.random.normal(size = final.shape[1])  #number_of_neurons) \n",
    "    return layer1_weights\n",
    "\n",
    "\n",
    "layer1_weights = middle_layer_1__inital_matrix()\n",
    "layer1_weights = np.transpose(np.matrix(layer1_weights))\n",
    "print(np.sum(layer1_weights))\n",
    "layer1_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cal():\n",
    "    y_result = y_train - zero_or_one_vec(sigmoid_vec(final *layer1_weights  ))\n",
    "    return np.transpose(final)*y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cal(n):\n",
    "    y_result = y_train[n] - zero_or_one_vec(sigmoid_vec(final[n] *layer1_weights  ))\n",
    "    return np.transpose(final[n])*y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]- zero_or_one_vec(sigmoid_vec(final[2] *layer1_weights  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13308"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(gradient_cal(112) == gradient_cal(121) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_cal(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13331, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_cal(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood():\n",
    "    return np.sum(np.multiply(y_train , (final * layer1_weights))) - np.sum(np.log(1+ np.exp((final *layer1_weights ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8317.766166719342"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_number = random.randint(0,final.shape[0])\n",
    "random_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8317.95353166903\n"
     ]
    }
   ],
   "source": [
    "n =gradient_cal(random_number)\n",
    "layer1_weights = layer1_weights + learning_rate* n\n",
    "print(log_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 5*(10**(-4))\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8314.789825531236\n",
      "-8308.401705741153\n",
      "-8306.420425707083\n",
      "-8308.923304280604\n",
      "-8302.030474482779\n",
      "-8305.599901788308\n",
      "-8303.915354838176\n",
      "-8303.24248688533\n",
      "-8302.957992550548\n",
      "-8305.282667291345\n",
      "-8301.147692376588\n",
      "-8302.021158082796\n",
      "-8299.996508238324\n",
      "-8297.821593859006\n",
      "-8298.009014248202\n",
      "-8297.185983163747\n",
      "-8296.269735691003\n",
      "-8297.50198445491\n",
      "-8297.26871132068\n",
      "-8297.80517188327\n",
      "-8296.045732114577\n",
      "-8296.769409510216\n",
      "-8292.078882312468\n",
      "-8296.550458364509\n",
      "-8296.113477998395\n",
      "-8296.531714781358\n",
      "-8294.050610387672\n",
      "-8298.433492023678\n",
      "-8293.654710708915\n",
      "-8296.766347608956\n",
      "-8295.796317462738\n",
      "-8292.450202772574\n",
      "-8291.574675332267\n",
      "-8296.331034406563\n",
      "-8295.641063168128\n",
      "-8292.989209953925\n",
      "-8294.812819462455\n",
      "-8294.186710619313\n",
      "-8291.199052602935\n",
      "-8292.949214390883\n",
      "-8290.490119342974\n",
      "-8291.870700094847\n",
      "-8292.719314313736\n",
      "-8294.787620918703\n",
      "-8292.389405205058\n",
      "-8290.85082162559\n",
      "-8294.09590035349\n",
      "-8292.801738241566\n",
      "-8294.355661506344\n",
      "-8291.039389975937\n",
      "-8294.10561970644\n",
      "-8292.28439061832\n",
      "-8291.645233380237\n",
      "-8292.166465573751\n",
      "-8290.768514730804\n",
      "-8290.69091019764\n",
      "-8289.709864997\n",
      "-8289.957421408188\n",
      "-8292.32644761529\n",
      "-8293.321417730833\n",
      "-8293.444205767983\n",
      "-8291.571311689095\n",
      "-8289.538284946071\n",
      "-8289.717757037934\n",
      "-8290.448613804958\n",
      "-8289.630266665188\n",
      "-8288.74948066037\n",
      "-8290.379045947935\n",
      "-8289.23187233857\n",
      "-8290.461031281844\n",
      "-8290.530279744782\n",
      "-8291.557789702661\n",
      "-8291.03348481477\n",
      "-8288.499849169719\n",
      "-8289.832710193656\n",
      "-8287.212498715067\n",
      "-8289.687417317253\n",
      "-8288.443733600163\n",
      "-8286.069396022334\n",
      "-8287.556532202469\n",
      "-8286.368969636713\n",
      "-8288.53595935195\n",
      "-8284.733590176082\n",
      "-8285.623806809057\n",
      "-8285.31877346547\n",
      "-8286.999077673725\n",
      "-8283.88827622779\n",
      "-8289.721750086355\n",
      "-8288.76965990729\n",
      "-8287.407587780952\n",
      "-8286.65642913705\n",
      "-8285.972610026258\n",
      "-8286.616570682849\n",
      "-8285.677265486\n",
      "-8286.638619860612\n",
      "-8286.918231198773\n",
      "-8281.563497814124\n",
      "-8289.18166756938\n",
      "-8286.097378795701\n",
      "-8288.031761462276\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    random_number = random.randint(0,final.shape[0]-1)\n",
    "    n =gradient_cal(random_number)\n",
    "    layer1_weights = layer1_weights + learning_rate* n\n",
    "    if(i%1000 == 0):\n",
    "        print(log_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8314.789825531236\n"
     ]
    }
   ],
   "source": [
    "print(log_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.0e-04],\n",
       "        [-2.0e-04],\n",
       "        [-3.0e-04],\n",
       "        ...,\n",
       "        [-5.0e-05],\n",
       "        [-5.0e-05],\n",
       "        [-2.5e-04]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5.e-05]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_weights[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_count_in_doc = {}\n",
    "size = len(x_test)\n",
    "for key, value in count_in_doc.items():\n",
    "    temp_count_in_doc[key] = [0]* size\n",
    "\n",
    "for k in range(len(x_test)):\n",
    "    i_long = tokenize(x_test[k])\n",
    "    for i in i_long:\n",
    "        if(i in temp_count_in_doc):\n",
    "            temp_count_in_doc[i][k] = (i_long.count(i))\n",
    "            \n",
    "\n",
    "l = []\n",
    "for key, value in temp_count_in_doc.items():\n",
    "    l.append(temp_count_in_doc[key])\n",
    "l = np.transpose(np.matrix(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001, 13331)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1= zero_or_one_vec(sigmoid_vec(l *layer1_weights  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.transpose(result1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('numbers_lr.csv', 'w')\n",
    "with f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\", \"Category\"])\n",
    "    n= 0 \n",
    "    for row in result:\n",
    "        writer.writerow([n ,row])\n",
    "        n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.0e-04],\n",
       "        [-2.0e-04],\n",
       "        [-3.0e-04],\n",
       "        ...,\n",
       "        [-5.0e-05],\n",
       "        [-5.0e-05],\n",
       "        [-2.5e-04]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.3\n",
      "0.6\n",
      "0.9\n",
      "1.2\n",
      "1.5\n",
      "1.8\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 500, 75):\n",
    "    print(i/250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "import emot\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class LR_v1:\n",
    "\n",
    "    # Change this value to change the learning rate\n",
    "    learning_rate = 5*(10**(-5))\n",
    "    # Steps to do stocastic Gradient descent by \n",
    "    steps = 100000\n",
    "\n",
    "    '''\n",
    "    This funtion intializes and sets the global variables for the whole class\n",
    "    '''\n",
    "    def __init__(self, x_train, x_test, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = np.matrix([y_train ])\n",
    "        self.y_train = np.transpose(self.y_train)\n",
    "        \n",
    "        self.x_test = x_test\n",
    "        self.X_dict = {}\n",
    "        self.X = self.document_count()\n",
    "        self.weights = self.weights()\n",
    "        self.training_data_log_likehood = []\n",
    "        # print(self.X.shape, self.)\n",
    "\n",
    "    # This makes a weights matrix to calculate train the matrix\n",
    "    def weights(self): \n",
    "        number_of_connections = self.X.shape[1]\n",
    "        weights =  np.zeros( number_of_connections)\n",
    "        # Un comment this for random non zero values on a normal distribution with median 1 \n",
    "        # layer1_weights = np.random.normal(size = final.shape[1]) \n",
    "        return np.transpose(np.matrix(weights))\n",
    "\n",
    "    # Tokenize funtion breaks the string into smalled word bits which can be used for the following steps to better understand the string\n",
    "    def tokenize(self, s):\n",
    "        #Lowercase every string\n",
    "        s = s.lower()\n",
    "        #convert emojis to text\n",
    "        s = emoji.demojize(s, delimiters=(\"\", \"\"))\n",
    "        s = emoji.demojize(s, delimiters=(\"\", \"\"))\n",
    "        answ = emot.emoticons(s)\n",
    "        if(str(type(answ))== \"<class 'list'>\"):\n",
    "            answ = answ[0]\n",
    "        if(answ['flag']):\n",
    "            # s = s.replace(answ['value'],answ['mean'])\n",
    "            j = 0\n",
    "            for i in answ['value']:\n",
    "                s = s.replace(i, \" \"+ answ['mean'][j].split()[-1])\n",
    "                j = j+1\n",
    "        # s = s.replace(ans['value'],ans['mean'])\n",
    "        # Remove punctuation and all weird characters\n",
    "        s = re.sub(\"\\W\",\" \", s)\n",
    "        # Special character cleaning\n",
    "        s = re.sub(\"\\s\",\" \", s)\n",
    "        return list(s.split())\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-1*x))\n",
    "\n",
    "    def sigmoid_vec(self,x):\n",
    "        return np.vectorize(self.sigmoid)(x)\n",
    "\n",
    "    def log_likelihood( self):\n",
    "        x = np.sum(np.multiply(self.y_train , (self.X * self.weights))) - np.sum(np.log(1+ np.exp((self.X * self.weights ))))\n",
    "        self.training_data_log_likehood.append(x)\n",
    "        return x \n",
    "\n",
    "    def zero_or_one(self,x):\n",
    "        if(x>(0.50)):\n",
    "            return 1 \n",
    "        return 0\n",
    "\n",
    "    def zero_or_one_vec(self,x):\n",
    "        return np.vectorize(self.zero_or_one)(x)\n",
    "\n",
    "    def gradient_cal(self):\n",
    "        y_result = self.y_train - self.zero_or_one_vec(self.sigmoid_vec(self.X * self.weights  ))\n",
    "        return np.transpose(self.X)*y_result\n",
    "    \n",
    "    def stochastic_gradient_cal(self, n ):\n",
    "        y_result = self.y_train[n] - self.zero_or_one_vec(self.sigmoid_vec(self.X[n] * self.weights  ))\n",
    "        return np.transpose(self.X[n])*y_result\n",
    "\n",
    "    # This function reads through the training data and populates a weight matrix shich storing the dictonary with weights in the self.X_dict variable \n",
    "    def document_count(self):\n",
    "        count_in_doc = {}\n",
    "        for x in self.x_train:\n",
    "            for i in self.tokenize(x):\n",
    "                if i not in count_in_doc:\n",
    "                    count_in_doc[i] = [0]*12000\n",
    "        count_in_doc[\"BIAS\"]= [1]*12000\n",
    "        for k in range(len(x_train)):\n",
    "            i_long = self.tokenize(self.x_train[k])\n",
    "            for i in i_long:\n",
    "                count_in_doc[i][k] = (i_long.count(i))\n",
    "        l = []\n",
    "        for key, value in count_in_doc.items():\n",
    "            l.append(count_in_doc[key])\n",
    "        l = np.transpose(np.matrix(l))\n",
    "        # print(l.shape)\n",
    "        self.X_dict = count_in_doc\n",
    "        return l\n",
    "    \n",
    "    def logistic_regression(self, learning_rate = 5*(10**(-5)) ,steps  =  100000):\n",
    "        for i in range(steps):\n",
    "            random_number = random.randint(0,self.X.shape[0]-1)\n",
    "            n =self.stochastic_gradient_cal(random_number)\n",
    "            self.weights = self.weights + learning_rate* n\n",
    "            if(i%1000 == 0):\n",
    "                print(i, self.log_likelihood())\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        temp_count_in_doc = {}\n",
    "        size = len(x_test)\n",
    "        for key, value in self.X_dict.items():\n",
    "            temp_count_in_doc[key] = [0]* size\n",
    "        temp_count_in_doc[\"BIAS\"] = [1]* size\n",
    "        for k in range(len(x_test)):\n",
    "            i_long = self.tokenize(x_test[k])\n",
    "            for i in i_long:\n",
    "                if(i in temp_count_in_doc):\n",
    "                    temp_count_in_doc[i][k] = (i_long.count(i))\n",
    "        # print(temp_count_in_doc)\n",
    "        l = []\n",
    "        for key, value in temp_count_in_doc.items():\n",
    "            l.append(temp_count_in_doc[key])\n",
    "        l = np.transpose(np.matrix(l))\n",
    "        # print(l.shape, self.weights.shape)\n",
    "        return self.zero_or_one_vec(self.sigmoid_vec(l  * self.weights  ))\n",
    "\n",
    "    def predict_write_to_file(self,x_test):\n",
    "        result = result = np.transpose(self.predict(x_test)).tolist()[0]\n",
    "        f = open('numbers_lr_v1.csv', 'w')\n",
    "        with f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Id\", \"Category\"])\n",
    "            n= 0 \n",
    "            for row in result:\n",
    "                writer.writerow([n ,row])\n",
    "                n = n+1\n",
    "\n",
    "    def f_score_one(self, x_dev, y_dev):\n",
    "        result = self.predict(x_dev)\n",
    "        n = (f1_score(y_dev, result, average = 'weighted'))\n",
    "        print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -8317.966187765905\n",
      "1000 -8317.319471470866\n",
      "2000 -8316.99878306368\n",
      "3000 -8316.607664844605\n",
      "4000 -8316.293802555245\n",
      "5000 -8316.540682870525\n",
      "6000 -8316.674289244023\n",
      "7000 -8316.118370984954\n",
      "8000 -8316.08433187938\n",
      "9000 -8315.749602843673\n",
      "10000 -8316.030944251748\n",
      "11000 -8315.902816118518\n",
      "12000 -8316.127968908835\n",
      "13000 -8315.914736683868\n",
      "14000 -8315.846283612478\n",
      "15000 -8315.90122123869\n",
      "16000 -8315.939491984647\n",
      "17000 -8316.151742024664\n",
      "18000 -8315.479358315573\n",
      "19000 -8315.82152046213\n",
      "20000 -8315.904177838156\n",
      "21000 -8315.948490665054\n",
      "22000 -8315.51019069707\n",
      "23000 -8315.749745280347\n",
      "24000 -8315.570067787623\n",
      "25000 -8315.683900139244\n",
      "26000 -8315.24784579086\n",
      "27000 -8315.50152410823\n",
      "28000 -8315.257334300079\n",
      "29000 -8315.600770780931\n",
      "30000 -8315.416610507102\n",
      "31000 -8315.161323082175\n",
      "32000 -8315.186967625259\n",
      "33000 -8315.224830835203\n",
      "34000 -8315.40883638832\n",
      "35000 -8315.677312574455\n",
      "36000 -8315.126453230618\n",
      "37000 -8314.964689777384\n",
      "38000 -8315.36659947561\n",
      "39000 -8315.490492480892\n",
      "40000 -8315.203161150843\n",
      "41000 -8315.067880142722\n",
      "42000 -8315.348103785389\n",
      "43000 -8315.143863707352\n",
      "44000 -8315.368392468317\n",
      "45000 -8315.0688125858\n",
      "46000 -8315.063654440164\n",
      "47000 -8315.465630296729\n",
      "48000 -8315.538476220463\n",
      "49000 -8315.303376681666\n",
      "50000 -8315.218787353197\n",
      "51000 -8314.929769533133\n",
      "52000 -8314.945098367756\n",
      "53000 -8315.251735751204\n",
      "54000 -8315.252165315014\n",
      "55000 -8315.031162888525\n",
      "56000 -8315.156399593698\n",
      "57000 -8315.141927442099\n",
      "58000 -8314.579581903547\n",
      "59000 -8315.288662290857\n",
      "60000 -8315.19720531466\n",
      "61000 -8314.8349738784\n",
      "62000 -8315.048716957861\n",
      "63000 -8314.989556217062\n",
      "64000 -8315.069358385505\n",
      "65000 -8315.045999261618\n",
      "66000 -8315.203004094763\n",
      "67000 -8314.99352030757\n",
      "68000 -8314.878528410867\n",
      "69000 -8314.862830149754\n",
      "70000 -8314.834045906346\n",
      "71000 -8314.888683948488\n",
      "72000 -8314.924413844461\n",
      "73000 -8314.60105234904\n",
      "74000 -8314.871236710598\n",
      "75000 -8314.797008742813\n",
      "76000 -8315.187196139977\n",
      "77000 -8314.874514497362\n",
      "78000 -8315.023753302517\n",
      "79000 -8314.75404589663\n",
      "80000 -8314.734457811135\n",
      "81000 -8315.018189612727\n",
      "82000 -8314.89745924423\n",
      "83000 -8315.08830392533\n",
      "84000 -8315.28931750213\n",
      "85000 -8314.959824516833\n",
      "86000 -8314.818914742811\n",
      "87000 -8314.77109529339\n",
      "88000 -8314.82242067188\n",
      "89000 -8314.890069150028\n",
      "90000 -8314.425573795883\n",
      "91000 -8314.863560945221\n",
      "92000 -8314.93112896226\n",
      "93000 -8314.88043541252\n",
      "94000 -8314.70354624522\n",
      "95000 -8314.734502497478\n",
      "96000 -8314.892497342838\n",
      "97000 -8314.993277806861\n",
      "98000 -8314.344066475775\n",
      "99000 -8315.001104174422\n"
     ]
    }
   ],
   "source": [
    "x_train = open('X_train.txt').readlines()\n",
    "y_train = open('Y_train.txt').readlines()\n",
    "y_train = [ 0 if x == \"0\\n\" else 1 for x in y_train] \n",
    "x_test = open('X_test.txt').readlines()\n",
    "x_dev= open('X_dev.txt').readlines()\n",
    "y_dev = open('y_dev.txt').readlines()\n",
    "y_dev = [ 0 if x == \"0\\n\" else 1 for x in y_dev] \n",
    "lr = LR_v1(x_train,x_test, y_train)\n",
    "lr.logistic_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_write_to_file(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78000594903676\n"
     ]
    }
   ],
   "source": [
    "lr.f_score_one( x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
